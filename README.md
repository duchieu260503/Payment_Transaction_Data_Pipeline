# Payment Transaction Data Pipeline

## ğŸ“Œ Project Overview
This project is an **ETL (Extract, Transform, Load) pipeline** for processing **payment transaction data** using **Apache Airflow**, **Docker**, and **PostgreSQL**. The pipeline performs the following steps:
1. **Extract:** Reads raw transaction data from a CSV file.
2. **Transform:** Applies data transformations (e.g., currency conversion, filtering, and cleaning).
3. **Load:** Inserts the processed data into a PostgreSQL database.

---

## ğŸš€ Project Setup

### **1ï¸âƒ£ Prerequisites**
Ensure you have the following installed on your local machine:
- **Docker & Docker Compose** â†’ [Install Docker](https://docs.docker.com/get-docker/)
- **WSL2 (For Windows users)** â†’ [Install WSL2](https://learn.microsoft.com/en-us/windows/wsl/install)
- **PostgreSQL (Running Locally or in Docker)** â†’ [Install PostgreSQL](https://www.postgresql.org/download/)

### **2ï¸âƒ£ Project Structure**
Ensure your local directory follows this structure:
```
Payment_Transaction_Data_Pipeline/
â”‚â”€â”€ dags/                          # DAGs folder (contains Python DAG scripts)
â”‚   â”œâ”€â”€ payment_etl.py             # Your DAG script
â”‚â”€â”€ data/                          # Folder to store CSV files
â”‚   â”œâ”€â”€ transactions.csv           # Raw transaction data
â”‚   â”œâ”€â”€ extracted.csv              # Extracted data (after extract step)
â”‚   â”œâ”€â”€ transformed.csv            # Transformed data (after transform step)
â”‚â”€â”€ logs/                          # Airflow logs
â”‚â”€â”€ plugins/                       # Custom Airflow plugins (if needed)
â”‚â”€â”€ docker-compose.yml             # Docker Compose config file
â”‚â”€â”€ requirements.txt               # Python dependencies (optional)
â”‚â”€â”€ README.md                      # Project documentation
```

### **3ï¸âƒ£ Setup Docker & Airflow**

#### **ğŸ“Œ Step 1: Modify `docker-compose.yml`**
Ensure `docker-compose.yml` has the correct volume mounts for **DAGs and data**:
```yaml
services:
  airflow_scheduler:
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
  airflow_webserver:
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
```

#### **ğŸ“Œ Step 2: Start Docker Containers**
Run the following commands to initialize Airflow:
```bash
docker compose down  # Stop any running containers
docker compose up -d  # Start Airflow in detached mode
```
Wait a few minutes for Airflow to initialize.

#### **ğŸ“Œ Step 3: Access Airflow UI**
Open your browser and go to:
```
http://localhost:8080
```
Default credentials:
- **Username:** `airflow`
- **Password:** `airflow`

#### **ğŸ“Œ Step 4: Verify DAGs & Trigger Execution**
Run the following command to check available DAGs:
```bash
docker exec -it airflow_scheduler airflow dags list
```
Trigger the DAG manually:
```bash
docker exec -it airflow_scheduler airflow dags trigger payment_etl
```

#### **ğŸ“Œ Step 5: View Logs**
Check logs for debugging:
```bash
docker exec -it airflow_scheduler airflow tasks logs payment_etl extract
```

---

## ğŸ› ï¸ PostgreSQL Database Setup
Ensure PostgreSQL is running **locally** or in **Docker**.

### **1ï¸âƒ£ Running PostgreSQL Locally**
If PostgreSQL is installed on your machine, update your database credentials in `payment_etl.py`:
```python
conn = psycopg2.connect("dbname=postgres user=postgres password=yourpassword host=localhost port=5432")
```

### **2ï¸âƒ£ Running PostgreSQL in Docker**
Alternatively, you can **run PostgreSQL in Docker**:
```bash
docker run --name postgres-db -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=yourpassword -p 5432:5432 -d postgres
```
Verify the container is running:
```bash
docker ps
```

Create a `transactions` table in PostgreSQL:
```sql
CREATE TABLE transactions (
    transaction_id INT PRIMARY KEY,
    timestamp TIMESTAMP,
    amount FLOAT,
    currency VARCHAR(10),
    payment_method VARCHAR(20),
    status VARCHAR(10)
);
```

---

## ğŸ”„ DAG Execution Steps
1ï¸âƒ£ **Extract:** Reads raw transactions from `/opt/airflow/data/transactions.csv`.
2ï¸âƒ£ **Transform:** Applies data transformations and saves the processed file.
3ï¸âƒ£ **Load:** Inserts the transformed data into **PostgreSQL**.

---

## ğŸ› ï¸ Debugging
If you run into issues, check logs:
```bash
docker logs airflow_scheduler  # View Airflow logs
docker logs airflow_webserver   # View Webserver logs
docker logs postgres-db         # View PostgreSQL logs
```

If DAGs are not appearing in the UI, restart Airflow:
```bash
docker compose restart
```

---

## ğŸ“Œ Next Steps
- **Enhance Data Processing** â†’ Add data validation and anomaly detection.
- **Deploy in Cloud** â†’ Run on AWS/GCP with managed Airflow & PostgreSQL.
- **Automate Alerts** â†’ Send Slack/email alerts for failed DAGs.

---

## ğŸ¤ Contributing
Feel free to submit pull requests or open issues!

**Happy Coding! ğŸš€**
